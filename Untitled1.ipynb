{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67993a4-66f1-4135-a8f8-e25b454667b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 277\u001b[0m\n\u001b[0;32m    275\u001b[0m model, train_loader \u001b[38;5;241m=\u001b[39m setup_lorenz_koopman()\n\u001b[0;32m    276\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 277\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_graph_koopman_model(model, train_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[3], line 134\u001b[0m, in \u001b[0;36mtrain_graph_koopman_model\u001b[1;34m(model, train_loader, epochs, device)\u001b[0m\n\u001b[0;32m    131\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Calculate all losses\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m total_loss, loss_components \u001b[38;5;241m=\u001b[39m criterion(model, x_seq, u_seq, edge_index, batch)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    137\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m, in \u001b[0;36mKoopmanGraphLoss.forward\u001b[1;34m(self, model, x_seq, u_seq, edge_index, batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m reconstructions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m---> 64\u001b[0m     g_t \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(x_seq[:, t], edge_index, batch)\n\u001b[0;32m     65\u001b[0m     x_recon_t \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(g_t)\n\u001b[0;32m     66\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mappend(g_t)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mGraphKoopmanEncoder.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, batch):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# Graph convolutions\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x, edge_index))\n\u001b[0;32m     16\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(h, edge_index))\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Global pooling\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m gcn_norm(  \u001b[38;5;66;03m# yapf: disable\u001b[39;00m\n\u001b[0;32m    242\u001b[0m         edge_index, edge_weight, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim),\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimproved, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_self_loops, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow, x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:109\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m idx \u001b[38;5;241m=\u001b[39m col \u001b[38;5;28;01mif\u001b[39;00m flow \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_to_target\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m row\n\u001b[0;32m    108\u001b[0m deg \u001b[38;5;241m=\u001b[39m scatter(edge_weight, idx, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dim_size\u001b[38;5;241m=\u001b[39mnum_nodes, reduce\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m deg_inv_sqrt \u001b[38;5;241m=\u001b[39m deg\u001b[38;5;241m.\u001b[39mpow_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    110\u001b[0m deg_inv_sqrt\u001b[38;5;241m.\u001b[39mmasked_fill_(deg_inv_sqrt \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    111\u001b[0m edge_weight \u001b[38;5;241m=\u001b[39m deg_inv_sqrt[row] \u001b[38;5;241m*\u001b[39m edge_weight \u001b[38;5;241m*\u001b[39m deg_inv_sqrt[col]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GraphKoopmanEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim=64):\n",
    "        super(GraphKoopmanEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Graph convolutions\n",
    "        h = torch.relu(self.conv1(x, edge_index))\n",
    "        h = torch.relu(self.conv2(h, edge_index))\n",
    "        # Global pooling\n",
    "        h = global_mean_pool(h, batch)\n",
    "        # Project to embedding space\n",
    "        return self.linear(h)\n",
    "\n",
    "class GraphKoopmanDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, output_dim, hidden_dim=64):\n",
    "        super(GraphKoopmanDecoder, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, g):\n",
    "        return self.net(g)\n",
    "\n",
    "class KoopmanGraphLoss(nn.Module):\n",
    "    def __init__(self, lambda_pred=1.0, lambda_metric=1.0):\n",
    "        super(KoopmanGraphLoss, self).__init__()\n",
    "        self.lambda_pred = lambda_pred\n",
    "        self.lambda_metric = lambda_metric\n",
    "        \n",
    "    def forward(self, model, x_seq, u_seq, edge_index, batch):\n",
    "        \"\"\"\n",
    "        Calculate all three losses for the Koopman GNN model\n",
    "        \n",
    "        Parameters:\n",
    "        - x_seq: Sequence of states [batch_size, seq_len, state_dim]\n",
    "        - u_seq: Sequence of controls [batch_size, seq_len-1, control_dim]\n",
    "        - edge_index: Graph connectivity\n",
    "        - batch: Batch indices for graphs\n",
    "        \n",
    "        Returns:\n",
    "        - total_loss: Combined loss value\n",
    "        - loss_dict: Dictionary containing individual loss components\n",
    "        \"\"\"\n",
    "        T = x_seq.size(1)\n",
    "        batch_size = x_seq.size(0)\n",
    "        \n",
    "        # 1. Auto-encoding Loss (Lae)\n",
    "        # Encode and decode each state\n",
    "        embeddings = []\n",
    "        reconstructions = []\n",
    "        for t in range(T):\n",
    "            g_t = model.encoder(x_seq[:, t], edge_index, batch)\n",
    "            x_recon_t = model.decoder(g_t)\n",
    "            embeddings.append(g_t)\n",
    "            reconstructions.append(x_recon_t)\n",
    "            \n",
    "        embeddings = torch.stack(embeddings, dim=1)  # [batch_size, T, embedding_dim]\n",
    "        reconstructions = torch.stack(reconstructions, dim=1)  # [batch_size, T, state_dim]\n",
    "        \n",
    "        Lae = torch.mean(torch.norm(reconstructions - x_seq, dim=-1))\n",
    "        \n",
    "        # 2. Prediction Loss (Lpred)\n",
    "        # Rollout in Koopman space\n",
    "        g_hat = embeddings[:, 0]  # Initial embedding\n",
    "        predicted_states = [model.decoder(g_hat)]\n",
    "        \n",
    "        for t in range(T-1):\n",
    "            # Apply Koopman operator: g_hat_next = K*g_hat + L*u\n",
    "            g_hat = model.koopman(g_hat) + torch.mm(u_seq[:, t], model.control_matrix)\n",
    "            predicted_states.append(model.decoder(g_hat))\n",
    "            \n",
    "        predicted_states = torch.stack(predicted_states, dim=1)\n",
    "        Lpred = torch.mean(torch.norm(predicted_states - x_seq, dim=-1))\n",
    "        \n",
    "        # 3. Metric Loss (Lmetric)\n",
    "        # Compute pairwise distances in both spaces\n",
    "        def compute_pairwise_distances(x):\n",
    "            n = x.size(0)\n",
    "            square = torch.sum(x**2, dim=-1, keepdim=True)\n",
    "            distances = square - 2 * torch.matmul(x, x.transpose(-2, -1)) + square.transpose(-2, -1)\n",
    "            return torch.sqrt(torch.clamp(distances, min=1e-12))\n",
    "        \n",
    "        Lmetric = 0\n",
    "        for t in range(T):\n",
    "            # Distances in state space\n",
    "            state_distances = compute_pairwise_distances(x_seq[:, t])\n",
    "            # Distances in Koopman space\n",
    "            koopman_distances = compute_pairwise_distances(embeddings[:, t])\n",
    "            # Compute metric loss\n",
    "            Lmetric += torch.mean(torch.abs(koopman_distances - state_distances))\n",
    "        \n",
    "        Lmetric = Lmetric / T\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = Lae + self.lambda_pred * Lpred + self.lambda_metric * Lmetric\n",
    "        \n",
    "        loss_dict = {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'autoencoding_loss': Lae.item(),\n",
    "            'prediction_loss': Lpred.item(),\n",
    "            'metric_loss': Lmetric.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, loss_dict\n",
    "\n",
    "def train_graph_koopman_model(model, train_loader, epochs=100, device='cpu'):\n",
    "    \"\"\"Train the Graph Koopman model with all three losses\"\"\"\n",
    "    criterion = KoopmanGraphLoss(lambda_pred=1.0, lambda_metric=1.0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        for batch_idx, (x_seq, u_seq, edge_index, batch) in enumerate(train_loader):\n",
    "            x_seq = x_seq.to(device)\n",
    "            u_seq = u_seq.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Calculate all losses\n",
    "            total_loss, loss_components = criterion(model, x_seq, u_seq, edge_index, batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses.append(loss_components)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            avg_losses = {k: sum(d[k] for d in epoch_losses) / len(epoch_losses)\n",
    "                         for k in epoch_losses[0].keys()}\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            for loss_name, loss_value in avg_losses.items():\n",
    "                print(f'{loss_name}: {loss_value:.4f}')\n",
    "            print('-' * 50)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class LorenzSystem:\n",
    "    def __init__(self, sigma=10.0, rho=28.0, beta=8/3):\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.beta = beta\n",
    "    \n",
    "    def derivatives(self, state):\n",
    "        \"\"\"Calculate Lorenz system derivatives\"\"\"\n",
    "        x, y, z = state[..., 0], state[..., 1], state[..., 2]\n",
    "        dx = self.sigma * (y - x)\n",
    "        dy = x * (self.rho - z) - y\n",
    "        dz = x * y - self.beta * z\n",
    "        return torch.stack([dx, dy, dz], dim=-1)\n",
    "    \n",
    "    def step(self, state, dt=0.01):\n",
    "        \"\"\"Perform one step of RK4 integration\"\"\"\n",
    "        k1 = self.derivatives(state)\n",
    "        k2 = self.derivatives(state + dt * k1/2)\n",
    "        k3 = self.derivatives(state + dt * k2/2)\n",
    "        k4 = self.derivatives(state + dt * k3)\n",
    "        return state + dt * (k1 + 2*k2 + 2*k3 + k4) / 6\n",
    "\n",
    "def generate_lorenz_data(batch_size, seq_length, dt=0.01, device='cpu'):\n",
    "    \"\"\"Generate batch of Lorenz trajectories\"\"\"\n",
    "    lorenz = LorenzSystem()\n",
    "    \n",
    "    # Random initial conditions\n",
    "    states = torch.randn(batch_size, 3).to(device) * 0.1\n",
    "    \n",
    "    # Generate trajectories\n",
    "    trajectories = [states]\n",
    "    for t in range(seq_length - 1):\n",
    "        states = lorenz.step(states, dt)\n",
    "        trajectories.append(states)\n",
    "    \n",
    "    trajectories = torch.stack(trajectories, dim=1)  # [batch_size, seq_length, 3]\n",
    "    \n",
    "    # Create dummy control inputs (zero for autonomous system)\n",
    "    controls = torch.zeros(batch_size, seq_length-1, 1).to(device)\n",
    "    \n",
    "    return trajectories, controls\n",
    "\n",
    "class LorenzGraphDataset:\n",
    "    def __init__(self, num_samples, seq_length, dt=0.01, num_nodes=10):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.dt = dt\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        # Create fixed graph structure (fully connected)\n",
    "        self.edge_index = torch.combinations(torch.arange(num_nodes), r=2).t()\n",
    "        # Add reverse edges\n",
    "        self.edge_index = torch.cat([self.edge_index, self.edge_index.flip(0)], dim=1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate single Lorenz trajectory\n",
    "        trajectory, controls = generate_lorenz_data(1, self.seq_length, self.dt)\n",
    "        \n",
    "        # Replicate the trajectory for each node (as node features)\n",
    "        x_seq = trajectory[0].unsqueeze(1).repeat(1, self.num_nodes, 1)\n",
    "        u_seq = controls[0]\n",
    "        \n",
    "        return x_seq, u_seq, self.edge_index\n",
    "\n",
    "def create_lorenz_dataloader(dataset, batch_size):\n",
    "    def collate_fn(batch):\n",
    "        x_seqs, u_seqs, edge_indices = zip(*batch)\n",
    "        \n",
    "        # Stack sequences\n",
    "        x_seqs = torch.stack(x_seqs)  # [batch_size, seq_len, num_nodes, 3]\n",
    "        u_seqs = torch.stack(u_seqs)  # [batch_size, seq_len-1, control_dim]\n",
    "        \n",
    "        # Create batched graph\n",
    "        batch_edge_index = []\n",
    "        batch_indices = []\n",
    "        offset = 0\n",
    "        for i, edge_index in enumerate(edge_indices):\n",
    "            batch_edge_index.append(edge_index + offset)\n",
    "            batch_indices.extend([i] * dataset.num_nodes)\n",
    "            offset += dataset.num_nodes\n",
    "            \n",
    "        batch_edge_index = torch.cat(batch_edge_index, dim=1)\n",
    "        batch_indices = torch.tensor(batch_indices)\n",
    "        \n",
    "        return x_seqs, u_seqs, batch_edge_index, batch_indices\n",
    "    \n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "class GraphKoopmanModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, control_dim=1):\n",
    "        super().__init__()\n",
    "        self.encoder = GraphKoopmanEncoder(input_dim, embedding_dim)\n",
    "        self.decoder = GraphKoopmanDecoder(embedding_dim, input_dim)\n",
    "        self.koopman = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.control_matrix = nn.Parameter(torch.randn(control_dim, embedding_dim))\n",
    "        \n",
    "def setup_lorenz_koopman(num_nodes=10, batch_size=32):\n",
    "    input_dim = 3\n",
    "    embedding_dim = 16\n",
    "    model = GraphKoopmanModel(input_dim, embedding_dim)\n",
    "    \n",
    "    dataset = LorenzGraphDataset(\n",
    "        num_samples=1000,\n",
    "        seq_length=50,\n",
    "        num_nodes=num_nodes\n",
    "    )\n",
    "    train_loader = create_lorenz_dataloader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    return model, train_loader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model, train_loader = setup_lorenz_koopman()\n",
    "    model = model.to(device)\n",
    "    trained_model = train_graph_koopman_model(model, train_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac7f86-b191-451f-8cec-9a812dcc81fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
